{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a14353b-46a1-4c60-9ee7-080174af9aa7",
   "metadata": {},
   "source": [
    "# Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8f10e2-148a-44e1-8cf6-748a75601885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_from_disk\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214c448-b292-4cd7-92e6-740be40bc11d",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959cac82-9842-4968-9597-4c8324936e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  1 11:29:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1070      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 27%   37C    P8              11W / 151W |    710MiB /  8192MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2720    C+G   ...Data\\Local\\Programs\\Blitz\\Blitz.exe    N/A      |\n",
      "|    0   N/A  N/A     12856    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     13724    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16048    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18744    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     20844    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21740    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     22368    C+G   ...aam7r\\AcrobatNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     23384    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     24280    C+G   ...ejd91yc\\AdobeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     25144    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26628    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     27644    C+G   ...Brave-Browser\\Application\\brave.exe    N/A      |\n",
      "|    0   N/A  N/A     28944    C+G   ...7.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     29716    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eceddb8b-556e-4e20-8137-a4e77efa0490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1345e-a55c-4636-b5ee-1078da5d812d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b6a852-9309-4923-848c-1eda295814ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa67ca-52c8-409e-8862-dac0997bafe0",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd958f-82f1-4f6b-9a00-97c41e225c30",
   "metadata": {},
   "source": [
    "## Generate dataset and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6d7c30-5744-4c0d-824d-dcc667e50a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, device):\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "\n",
    "    def deserialize_array(self, binary_string, dtype, shape):\n",
    "        return np.frombuffer(binary_string, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        # retrieve values\n",
    "        student_id = data['student_id']\n",
    "        prompt_id = data['prompt_id']\n",
    "        sentence = data['sentence']\n",
    "        t_q_a_embeddings = torch.tensor(self.deserialize_array(data['t_q_a_embeddings'], np.float32, (768,))).to(self.device)\n",
    "        embeddings_text = torch.tensor(self.deserialize_array(data['embeddings_text'], np.float32, (768,))).to(self.device)\n",
    "        content = torch.tensor(data['content']).to(self.device)\n",
    "        wording = torch.tensor(data['wording']).to(self.device)\n",
    "        normalized_content = torch.tensor(data['normalized_content']).to(self.device)\n",
    "        normalized_wording = torch.tensor(data['normalized_wording']).to(self.device)\n",
    "        normalized_lexical_density = torch.tensor(data['normalized_lexical_density']).unsqueeze(0).to(self.device)\n",
    "        normalized_spell_checker = torch.tensor(data['normalized_spell_checker']).unsqueeze(0).to(self.device)\n",
    "        normalized_tf_idf_question_score = torch.tensor(data['normalized_tf_idf_question_score']).unsqueeze(0).to(self.device)\n",
    "        normalized_avg_word_length = torch.tensor(data['normalized_avg_word_length']).unsqueeze(0).to(self.device)\n",
    "        normalized_smog_index = torch.tensor(data['normalized_smog_index']).unsqueeze(0).to(self.device)\n",
    "        normalized_coleman_liau_index = torch.tensor(data['normalized_coleman_liau_index']).unsqueeze(0).to(self.device)\n",
    "        normalized_flesch_reading_ease = torch.tensor(data['normalized_flesch_reading_ease']).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'student_id': student_id,\n",
    "            'prompt_id': prompt_id,\n",
    "            'sentence': sentence,\n",
    "            't_q_a_embeddings': t_q_a_embeddings,\n",
    "            'embeddings_text': embeddings_text,\n",
    "            'content': content,\n",
    "            'wording': wording,\n",
    "            'normalized_content': normalized_content,\n",
    "            'normalized_wording': normalized_wording,\n",
    "            'normalized_lexical_density': normalized_lexical_density,\n",
    "            'normalized_spell_checker': normalized_spell_checker,\n",
    "            'normalized_tf_idf_question_score': normalized_tf_idf_question_score,\n",
    "            'normalized_avg_word_length': normalized_avg_word_length,\n",
    "            'normalized_smog_index': normalized_smog_index,\n",
    "            'normalized_coleman_liau_index': normalized_coleman_liau_index,\n",
    "            'normalized_flesch_reading_ease': normalized_flesch_reading_ease,\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        return self.dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843e316c-df6c-485f-8561-6f890c556f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderFactory():\n",
    "\n",
    "    def __init__(self, path:str = './data/hugging_face', batch_size = 12, device = 'cpu'):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = load_from_disk(path, keep_in_memory=True)\n",
    "        self.device = device\n",
    "\n",
    "        print(\"1. Loading dataset: ...\", end=\"\")\n",
    "        dataset = load_from_disk(path, keep_in_memory=True)\n",
    "        print(\"\\r1. Loading dataset: done ✔️\")\n",
    "\n",
    "        print(\"2. Split datasets: ...\", end=\"\")\n",
    "        train_validation_splits = self.dataset['train'].train_test_split(test_size=0.2)\n",
    "        print(\"\\r2. Preprocess datasets: done ✔️\")\n",
    "\n",
    "        print(\"3. Split datasets: ...\", end=\"\")\n",
    "        self.train_data = PromptDataset(train_validation_splits['train'], self.device)\n",
    "        self.val_data = PromptDataset(train_validation_splits['test'], self.device)\n",
    "        print(\"\\r3. Split datasets: done ✔️\")\n",
    "\n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int :\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\\t-> {len(self.train_data)/self.batch_size}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\\t\\t-> {len(self.val_data)/self.batch_size}\")\n",
    "        total = len(self.train_data) + len(self.val_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        else:\n",
    "            dataloader = self.dataloader_val\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v for k, v in batch.items()}\n",
    "            yield batch_on_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f4fc8-1830-4fab-ae74-d12fb945f374",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67182de-ccb8-4f77-87e0-5c88be245435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading dataset: done ✔️\n",
      "2. Preprocess datasets: done ✔️\n",
      "3. Split datasets: done ✔️\n",
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 5732\t-> 238.83333333333334\n",
      "Validation\t: 1433\t\t-> 59.708333333333336\n",
      "Total\t\t: 7165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7165"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataLoaderFactory(device=device, batch_size=BATCH_SIZE)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e512f-1fad-4c54-80c5-c0500a25f086",
   "metadata": {},
   "source": [
    "### Testing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fded6cee-24b5-4d28-a3df-a857fd1336c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "nb = next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae8f919-4ba2-46be-be03-d47b248ddd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student_id:              24\n",
      "prompt_id:               24\n",
      "sentence:                24\n",
      "t_q_a_embeddings:        torch.Size([24, 768])\n",
      "embeddings_text:         torch.Size([24, 768])\n",
      "content:                 torch.Size([24])\n",
      "wording:                 torch.Size([24])\n",
      "normalized_content:      torch.Size([24])\n",
      "normalized_wording:      torch.Size([24])\n",
      "normalized_lexical_density:torch.Size([24, 1])\n",
      "normalized_spell_checker:torch.Size([24, 1])\n",
      "normalized_tf_idf_question_score:torch.Size([24, 1])\n",
      "normalized_avg_word_length:torch.Size([24, 1])\n",
      "normalized_smog_index:   torch.Size([24, 1])\n",
      "normalized_coleman_liau_index:torch.Size([24, 1])\n",
      "normalized_flesch_reading_ease:torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'student_id:':<25}{len(nb['student_id'])}\")\n",
    "print(f\"{'prompt_id:':<25}{len(nb['prompt_id'])}\")\n",
    "print(f\"{'sentence:':<25}{len(nb['sentence'])}\")\n",
    "print(f\"{'t_q_a_embeddings:':<25}{nb['t_q_a_embeddings'].shape}\")\n",
    "print(f\"{'embeddings_text:':<25}{nb['embeddings_text'].shape}\")\n",
    "print(f\"{'content:':<25}{nb['content'].shape}\")\n",
    "print(f\"{'wording:':<25}{nb['wording'].shape}\")\n",
    "print(f\"{'normalized_content:':<25}{nb['normalized_content'].shape}\")\n",
    "print(f\"{'normalized_wording:':<25}{nb['normalized_wording'].shape}\")\n",
    "print(f\"{'normalized_lexical_density:':<25}{nb['normalized_lexical_density'].shape}\")\n",
    "print(f\"{'normalized_spell_checker:':<25}{nb['normalized_spell_checker'].shape}\")\n",
    "print(f\"{'normalized_tf_idf_question_score:':<25}{nb['normalized_tf_idf_question_score'].shape}\")\n",
    "print(f\"{'normalized_avg_word_length:':<25}{nb['normalized_avg_word_length'].shape}\")\n",
    "print(f\"{'normalized_smog_index:':<25}{nb['normalized_smog_index'].shape}\")\n",
    "print(f\"{'normalized_coleman_liau_index:':<25}{nb['normalized_coleman_liau_index'].shape}\")\n",
    "print(f\"{'normalized_flesch_reading_ease:':<25}{nb['normalized_flesch_reading_ease'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d85e7-b1e4-4d22-b043-ee9374286d04",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c70332-9c16-499c-9fc3-734f72e2ecf4",
   "metadata": {},
   "source": [
    "1. **Input Layer:** Accept the embeddings of the question, text, and answer. This would result in three separate input layers, each of dimension `[batch_size, embedding_size]`.\n",
    "2. **Concatenate Layer:** Concatenate the embeddings along the feature axis. This results in `[batch_size, 3 * embedding_size]` dimensions.\n",
    "3. **Dense Layers:** Add a couple of Dense layers with suitable dropout layers in between for regularization. For instance:\n",
    "    - Dense layer with `1024` units, `ReLU` activation.\n",
    "    - Dropout layer with rate `0.5`.\n",
    "    - Dense layer with `512` units, ReLU activation.\n",
    "    - Another Dropout layer with rate `0.5`.\n",
    "4. **Output Layer:** Single Dense layer with `2` units (since you want two values between -1 and 1) and a `tanh` activation function.\n",
    "5. **Loss Function:** If the output of your model is a vector with two values between -1 and 1, then you're essentially dealing with multi-output regression. You can use `Mean Squared Error` or `Mean Absolute Error`, but it'll be applied to each output.\n",
    "6. **Optimizer:** Remains the same; options include `Adam`, `RMSprop`, and `SGD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a39cc-86ee-419b-b045-12f1a9d8dac1",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9565edec-5034-4e98-baa3-7f9e8c466857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaV3Batch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DebertaV3Batch, self).__init__()\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "    \n",
    "\n",
    "    def encode(self, sentences):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Only take the embeddings of the [CLS] token (or use the mean of the token embeddings if desired)\n",
    "        sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ee62c9-4c41-479d-a208-0e508c11e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaV3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DebertaV3, self).__init__()\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "\n",
    "    def sliding_window(self, text, window_size=512, overlap_size=256):\n",
    "        tokenized_text = self.tokenizer(text, return_tensors=\"pt\", truncation=False, padding=False)[\"input_ids\"][0]\n",
    "        windows = []\n",
    "        for start_idx in range(0, len(tokenized_text) - window_size + 1, window_size - overlap_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            windows.append(tokenized_text[start_idx:end_idx])\n",
    "        return windows\n",
    "\n",
    "    def process_windows(self, windows):\n",
    "        embeddings = []\n",
    "        for window in windows:\n",
    "            inputs = {\"input_ids\": window.unsqueeze(0)}\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embedding)\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    def encode(self, sentences):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        if len(encoded_input['input_ids'][0]) > 512:\n",
    "            windows = self.sliding_window(text)\n",
    "            window_embeddings = self.process_windows(windows)\n",
    "            \n",
    "            # Optionally, you can aggregate the window embeddings to get a single embedding for the entire text\n",
    "            # For example, by averaging:\n",
    "            average_embedding = torch.mean(torch.cat(window_embeddings), dim=0)\n",
    "            return average_embedding\n",
    "        else:\n",
    "            # Only take the embeddings of the [CLS] token (or use the mean of the token embeddings if desired)\n",
    "            sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            return sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9381011-354c-4a66-aa20-565a15ff9bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "deberta = DebertaV3()\n",
    "debertaBatch = DebertaV3Batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151c3c0-9d49-423a-9347-e7ed526bdf18",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c51e654-671e-496f-aa2d-c6668fdd8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Model\n",
    "# class QA_Score_Model(nn.Module):\n",
    "#     def __init__(self, input_dim=3*768+7, dropout_rate=0.5):\n",
    "#         super(QA_Score_Model, self).__init__()\n",
    "\n",
    "#         # Dense Layers\n",
    "#         self.fc1 = nn.Linear(input_dim, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.fc3 = nn.Linear(512, 2)  # Output 2 values for each instance\n",
    "\n",
    "#         # Dropout\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         if self.training:\n",
    "#             x = self.dropout(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         if self.training:\n",
    "#             x = self.dropout(x)\n",
    "#         x = torch.tanh(self.fc3(x))  # tanh to get outputs in range [-1, 1]\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd958cb-ca28-4a23-9910-93c28ac51584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Score_Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate=0.5):\n",
    "        super(QA_Score_Model, self).__init__()\n",
    "\n",
    "        # Embedding processors\n",
    "        self.fc_text = nn.Linear(embedding_dim, 256)\n",
    "        self.fc_t_q_a = nn.Linear(embedding_dim, 256)\n",
    "\n",
    "        # Scalar Processors\n",
    "        self.fc_lexical = nn.Linear(1, 16)\n",
    "        self.fc_spell = nn.Linear(1, 16)\n",
    "        self.fc_tfidf = nn.Linear(1, 16)\n",
    "        self.fc_avg_word_length = nn.Linear(1, 16)\n",
    "        self.fc_smog = nn.Linear(1, 16)\n",
    "        self.fc_coleman = nn.Linear(1, 16)\n",
    "        self.fc_flesch = nn.Linear(1, 16)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Shared Dense Layers\n",
    "        self.fc1 = nn.Linear(256*2 + 16*7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        # Output Layers\n",
    "        self.out_content = nn.Linear(256, 1)\n",
    "        self.out_wording = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, text, t_q_a, features):\n",
    "        lexical, spell, tfidf, avg_word_length, smog, coleman, flesch = features\n",
    "        # Pass through convolutional layers\n",
    "        x_text = F.relu(self.fc_text(text))\n",
    "        x_t_q_a = F.relu(self.fc_t_q_a(t_q_a))\n",
    "\n",
    "        # Pass through scalar processors\n",
    "        x_lexical = F.relu(self.fc_lexical(lexical))\n",
    "        x_spell = F.relu(self.fc_spell(spell))\n",
    "        x_tfidf = F.relu(self.fc_tfidf(tfidf))\n",
    "        x_avg_word_length = F.relu(self.fc_avg_word_length(avg_word_length))\n",
    "        x_smog = F.relu(self.fc_smog(smog))\n",
    "        x_coleman = F.relu(self.fc_coleman(coleman))\n",
    "        x_flesch = F.relu(self.fc_flesch(flesch))\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat((x_text, x_t_q_a, x_lexical, x_spell, x_tfidf, x_avg_word_length, x_smog, x_coleman, x_flesch), dim=1)\n",
    "        \n",
    "        # Pass through shared dense layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output\n",
    "        content = self.out_content(x)\n",
    "        wording = self.out_wording(x)\n",
    "        \n",
    "        return torch.cat((content, wording), dim=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d2d50-f523-4794-b993-49bf5497a153",
   "metadata": {},
   "source": [
    "### Create an instance of the model, loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11708a30-f4f4-41bc-b759-918d56b374a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model, loss function and the optimizer\n",
    "model = QA_Score_Model(768)\n",
    "model = model.to(device)\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error less sensitive to outliers # MSELoss Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa399d64-ba3a-41b4-ae4d-d699d3a6be37",
   "metadata": {},
   "source": [
    "### MCRMSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e23974-ac03-4d23-a34a-251cf416c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCRMSE(targets, predictions):\n",
    "    \"\"\"\n",
    "    Compute the Mean Columnwise Root Mean Squared Error.\n",
    "    \n",
    "    Parameters:\n",
    "    - targets: Actual target values\n",
    "    - predictions: Model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    - Mean Columnwise RMSE\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    mse = torch.mean((targets - predictions) ** 2, dim=0)\n",
    "    \n",
    "    # Take square root of each column's MSE\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute mean of RMSE across all columns\n",
    "    mcrmse = torch.mean(rmse)\n",
    "    \n",
    "    return mcrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76493c03-5a20-4658-80c6-31a1bcb5c41f",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c3c6ec-b433-4efa-a5a5-8e8324d87836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataset, epoch):\n",
    "    model = model.train()\n",
    "    \n",
    "    total_train_loss = 0.0\n",
    "    total_train_mcrmse = 0.0\n",
    "    \n",
    "    batchs = dataset.get_batch('train')\n",
    "    \n",
    "    num_train_batches = len(dataset.train_data) // BATCH_SIZE\n",
    "    for i, batch in enumerate(batchs):\n",
    "        # Get the batch data\n",
    "        student_id = batch['student_id']\n",
    "        prompt_id = batch['prompt_id']\n",
    "        t_q_a_embeddings = batch['t_q_a_embeddings']\n",
    "        embeddings_text = batch['embeddings_text']\n",
    "        content = batch['content']\n",
    "        wording = batch['wording']\n",
    "        normalized_content = batch['normalized_content']\n",
    "        normalized_wording = batch['normalized_wording']\n",
    "        normalized_lexical_density = batch['normalized_lexical_density']\n",
    "        normalized_spell_checker = batch['normalized_spell_checker']\n",
    "        normalized_tf_idf_question_score = batch['normalized_tf_idf_question_score']\n",
    "        normalized_avg_word_length = batch['normalized_avg_word_length']\n",
    "        normalized_smog_index = batch['normalized_smog_index']\n",
    "        normalized_coleman_liau_index = batch['normalized_coleman_liau_index']\n",
    "        normalized_flesch_reading_ease = batch['normalized_flesch_reading_ease']\n",
    "        \n",
    "        # create input targets\n",
    "        # input_data = torch.cat(\n",
    "        #     (\n",
    "        #         embeddings_question,\n",
    "        #         embeddings_text,\n",
    "        #         embeddings_text,\n",
    "        #         normalized_lexical_density,\n",
    "        #         normalized_spell_checker,\n",
    "        #         normalized_tf_idf_question_score,\n",
    "        #         normalized_avg_word_length,\n",
    "        #         normalized_smog_index,\n",
    "        #         normalized_coleman_liau_index,\n",
    "        #         normalized_flesch_reading_ease\n",
    "        #     ), dim=1).to(device)\n",
    "        targets = torch.stack(\n",
    "            (\n",
    "                normalized_content,\n",
    "                normalized_wording\n",
    "            ), dim=1).to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = (normalized_lexical_density, \\\n",
    "                    normalized_spell_checker, \\\n",
    "                    normalized_tf_idf_question_score, \\\n",
    "                    normalized_avg_word_length, \\\n",
    "                    normalized_smog_index, \\\n",
    "                    normalized_coleman_liau_index, \\\n",
    "                    normalized_flesch_reading_ease \\\n",
    "                   )\n",
    "        outputs = model(embeddings_text, t_q_a_embeddings, features)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        eval_metric = MCRMSE(targets, outputs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss (you might want to print every N batches, not every batch)\n",
    "        print(f\"\\rEpoch [{epoch + 1}/{EPOCHS}], Batch [{i}/{num_train_batches}], Loss: {loss.item()}, MCRMSE: {eval_metric.item()}\", end=\"\")\n",
    "        \n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_mcrmse += eval_metric.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "    avg_train_mcrmse = total_train_mcrmse / num_train_batches\n",
    "\n",
    "    return model, avg_train_loss, avg_train_mcrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938650b-cf46-4e24-88ea-1d46cc827f3f",
   "metadata": {},
   "source": [
    "### Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b820d0-8dd1-4292-9806-4d20186bb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, dataset, epoch):\n",
    "    print(f\"\\nValidating...\", end=\"\")\n",
    "    # Validation Evaluation\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    total_val_loss = 0.0\n",
    "    total_val_mcrmse = 0.0\n",
    "    \n",
    "    val_batches = dataset.get_batch('val')\n",
    "    \n",
    "    num_val_batches = len(dataset.val_data) // BATCH_SIZE\n",
    "    with torch.no_grad():  # Disable gradient computations\n",
    "        for i, val_batch in enumerate(val_batches):\n",
    "            student_id = val_batch['student_id']\n",
    "            prompt_id = val_batch['prompt_id']\n",
    "            t_q_a_embeddings = val_batch['t_q_a_embeddings']\n",
    "            embeddings_text = val_batch['embeddings_text']\n",
    "            content = val_batch['content']\n",
    "            wording = val_batch['wording']\n",
    "            normalized_content = val_batch['normalized_content']\n",
    "            normalized_wording = val_batch['normalized_wording']\n",
    "            normalized_lexical_density = val_batch['normalized_lexical_density']\n",
    "            normalized_spell_checker = val_batch['normalized_spell_checker']\n",
    "            normalized_tf_idf_question_score = val_batch['normalized_tf_idf_question_score']\n",
    "            normalized_avg_word_length = val_batch['normalized_avg_word_length']\n",
    "            normalized_smog_index = val_batch['normalized_smog_index']\n",
    "            normalized_coleman_liau_index = val_batch['normalized_coleman_liau_index']\n",
    "            normalized_flesch_reading_ease = val_batch['normalized_flesch_reading_ease']\n",
    "\n",
    "            # Create input and targets\n",
    "            # val_input_data = torch.cat(\n",
    "            #     (\n",
    "            #         embeddings_question,\n",
    "            #         embeddings_question,\n",
    "            #         text,\n",
    "            #         normalized_lexical_density,\n",
    "            #         normalized_spell_checker,\n",
    "            #         normalized_tf_idf_question_score,\n",
    "            #         normalized_avg_word_length,\n",
    "            #         normalized_smog_index,\n",
    "            #         normalized_coleman_liau_index,\n",
    "            #         normalized_flesch_reading_ease\n",
    "            #     ), dim=1).to(device)\n",
    "            val_targets = torch.stack((normalized_content, normalized_wording), dim=1).to(device)\n",
    "            \n",
    "            features = (normalized_lexical_density, \\\n",
    "                        normalized_spell_checker, \\\n",
    "                        normalized_tf_idf_question_score, \\\n",
    "                        normalized_avg_word_length, \\\n",
    "                        normalized_smog_index, \\\n",
    "                        normalized_coleman_liau_index, \\\n",
    "                        normalized_flesch_reading_ease \\\n",
    "                       )\n",
    "            # Forward pass\n",
    "            val_outputs = model(embeddings_text, t_q_a_embeddings, features)\n",
    "\n",
    "            # Compute loss and MCRMSE\n",
    "            val_loss = criterion(val_outputs, val_targets)\n",
    "            val_mcrmse = MCRMSE(val_targets, val_outputs)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            total_val_mcrmse += val_mcrmse.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_val_batches\n",
    "    avg_val_mcrmse = total_val_mcrmse / num_val_batches\n",
    "\n",
    "    print(f\"\\rValidation Loss after Epoch {epoch + 1}: {avg_val_loss}, Avg MCRMSE: {avg_val_mcrmse}\\n\")\n",
    "    \n",
    "    return avg_val_loss, avg_val_mcrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "892f78e8-72a3-48ab-8deb-d3443311b6e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [238/238], Loss: 0.25973179936408997, MCRMSE: 0.31720280647277834\n",
      "Validation Loss after Epoch 1: 0.2682429701089859, Avg MCRMSE: 0.3388161431934874\n",
      "\n",
      "Epoch [2/100], Batch [238/238], Loss: 0.25248727202415466, MCRMSE: 0.31985861063003545\n",
      "Validation Loss after Epoch 2: 0.26209523844516885, Avg MCRMSE: 0.33074341865919404\n",
      "\n",
      "Epoch [3/100], Batch [238/238], Loss: 0.2335994690656662, MCRMSE: 0.307933896780014044\n",
      "Validation Loss after Epoch 3: 0.2536515207108805, Avg MCRMSE: 0.3199382861286907\n",
      "\n",
      "Epoch [4/100], Batch [238/238], Loss: 0.18511031568050385, MCRMSE: 0.22136232256889343\n",
      "Validation Loss after Epoch 4: 0.24474942229561886, Avg MCRMSE: 0.3074447984917689\n",
      "\n",
      "Epoch [5/100], Batch [238/238], Loss: 0.2413213551044464, MCRMSE: 0.303460240364074742\n",
      "Validation Loss after Epoch 5: 0.23173163351366075, Avg MCRMSE: 0.2894761764397055\n",
      "\n",
      "Epoch [6/100], Batch [238/238], Loss: 0.2354806512594223, MCRMSE: 0.285915583372116166\n",
      "Validation Loss after Epoch 6: 0.22078117349390255, Avg MCRMSE: 0.27543420736062324\n",
      "\n",
      "Epoch [7/100], Batch [238/238], Loss: 0.1535780429840088, MCRMSE: 0.199800625443458564\n",
      "Validation Loss after Epoch 7: 0.20874658782603378, Avg MCRMSE: 0.2606907979411594\n",
      "\n",
      "Epoch [8/100], Batch [238/238], Loss: 0.20034341514110565, MCRMSE: 0.26415520906448364\n",
      "Validation Loss after Epoch 8: 0.2041558363174988, Avg MCRMSE: 0.2543964194039167\n",
      "\n",
      "Epoch [9/100], Batch [238/238], Loss: 0.18037645518779755, MCRMSE: 0.21787530183792114\n",
      "Validation Loss after Epoch 9: 0.1986595094203949, Avg MCRMSE: 0.24840598970146502\n",
      "\n",
      "Epoch [10/100], Batch [170/238], Loss: 0.21267983317375183, MCRMSE: 0.27100431919097913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "counter = 0  # To count number of epochs with no improvement\n",
    "best_val_loss = None  # To keep track of best validation loss\n",
    "best_model_state = None  # To store best model's state\n",
    "\n",
    "train_losses = []\n",
    "train_mcrmse = []\n",
    "val_losses = []\n",
    "val_mcrmse = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # training loop\n",
    "    model, epoch_train_loss, epoch_train_mcrmse = training_loop(model, dataset, epoch)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_mcrmse.append(epoch_train_mcrmse)\n",
    "    \n",
    "    # validation loop\n",
    "    epoch_val_loss, epoch_val_mcrmse = validation_loop(model, dataset, epoch)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_mcrmse.append(epoch_val_mcrmse)\n",
    "\n",
    "    # Check if this epoch's validation loss is the best we've seen so far.\n",
    "    if best_val_loss is None or epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())  # Save best model\n",
    "        counter = 0  # Reset counter\n",
    "    else:\n",
    "        counter += 1  # Increment counter as no improvement in validation loss\n",
    "\n",
    "    # If we've had patience number of epochs without improvement, stop training\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping on epoch {epoch}. Best epoch was {epoch - counter} with val loss: {best_val_loss:.4f}.\")\n",
    "        break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_state)\n",
    "torch.save(model.state_dict(), './out/best_model.pt')  # Save the best model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f186fe-aa77-4f32-9332-b46a53d6407b",
   "metadata": {},
   "source": [
    "### Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4c6a2-d3e5-474d-9a69-b60e59513a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(train_losses, val_losses, train_mcrmse, val_mcrmse):\n",
    "    # Monokai color palette\n",
    "    colors = ['#F92672', '#A6E22E', '#66D9EF', '#FD971F']\n",
    "    \n",
    "    # Set the background color and grid color\n",
    "    sns.set_style(\"darkgrid\", {\n",
    "        \"axes.facecolor\": \".1\",  # Dark background\n",
    "        \"grid.color\": \".15\",  # Slightly grayish grid lines\n",
    "        \"axes.labelcolor\": \"white\",\n",
    "        \"xtick.color\": \"white\",\n",
    "        \"ytick.color\": \"white\"\n",
    "    })\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plotting the losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Set entire background color\n",
    "    fig = plt.gcf()\n",
    "    fig.patch.set_facecolor('.1')  # Color for the whole backgroun\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=epochs, y=train_losses, label='Train Loss', marker='o', color=colors[0])\n",
    "    sns.lineplot(x=epochs, y=val_losses, label='Validation Loss', marker='o', color=colors[1])\n",
    "    plt.title('Training and Validation Loss', color='white')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    legend = plt.legend()\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(\"white\")  # Set the legend text color to white\n",
    "\n",
    "    # Plotting the MCRMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=epochs, y=train_mcrmse, label='Train MCRMSE', marker='o', color=colors[2])\n",
    "    sns.lineplot(x=epochs, y=val_mcrmse, label='Validation MCRMSE', marker='o', color=colors[3])\n",
    "    plt.title('Training and Validation MCRMSE', color='white')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MCRMSE')\n",
    "    legend = plt.legend()\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(\"white\")  # Set the legend text color to white\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a845c-f61e-4ae4-9a69-58b0c85f6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, call the function to plot\n",
    "plot_graphs(train_losses, val_losses, train_mcrmse, val_mcrmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28171c7-d418-404a-a7e7-cfb3567c7c12",
   "metadata": {},
   "source": [
    "## Saving torch script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58131d-0d65-4e15-bd16-c67008f19d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./out/best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21393b-2e29-436e-bc9e-bc93f32469cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple_batches = dataset.get_batch('train')\n",
    "ex_batch = next(exemple_batches)\n",
    "t_q_a_embeddings = ex_batch['t_q_a_embeddings']\n",
    "embeddings_text = ex_batch['embeddings_text']\n",
    "embeddings_answer = ex_batch['text']\n",
    "normalized_content = ex_batch['normalized_content']\n",
    "normalized_wording = ex_batch['normalized_wording']\n",
    "normalized_lexical_density = ex_batch['normalized_lexical_density']\n",
    "normalized_spell_checker = ex_batch['normalized_spell_checker']\n",
    "normalized_tf_idf_question_score = ex_batch['normalized_tf_idf_question_score']\n",
    "normalized_avg_word_length = ex_batch['normalized_avg_word_length']\n",
    "normalized_smog_index = ex_batch['normalized_smog_index']\n",
    "normalized_coleman_liau_index = ex_batch['normalized_coleman_liau_index']\n",
    "normalized_flesch_reading_ease = ex_batch['normalized_flesch_reading_ease']\n",
    "# example_input_tensor = torch.cat(\n",
    "#     (\n",
    "#         embeddings_question,\n",
    "#         embeddings_text,\n",
    "#         text,\n",
    "#         normalized_lexical_density,\n",
    "#         normalized_spell_checker,\n",
    "#         normalized_tf_idf_question_score,\n",
    "#         normalized_avg_word_length,\n",
    "#         normalized_smog_index,\n",
    "#         normalized_coleman_liau_index,\n",
    "#         normalized_flesch_reading_ease\n",
    "#     ), dim=1)\n",
    "\n",
    "features = (normalized_lexical_density.to('cpu'), \\\n",
    "            normalized_spell_checker.to('cpu'), \\\n",
    "            normalized_tf_idf_question_score.to('cpu'), \\\n",
    "            normalized_avg_word_length.to('cpu'), \\\n",
    "            normalized_smog_index.to('cpu'), \\\n",
    "            normalized_coleman_liau_index.to('cpu'), \\\n",
    "            normalized_flesch_reading_ease.to('cpu') \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f85c4d-47fb-42d5-8b08-e90af1199874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model.to('cpu'), (embeddings_text.to('cpu'), t_q_a_embeddings.to('cpu'), features))\n",
    "# Save the traced model\n",
    "traced_model.save(\"./out/best_model_script.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860f40b-e9c7-4f20-9c9f-71ca695f20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad3e0e-5421-4bf2-9b25-6a77b4c40f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_round(x, decimals = 3):\n",
    "    multiplicator = 10**decimals\n",
    "    return torch.round(x * 1000) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8782aac0-bd1d-433b-b849-f4b784bb41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1, tensor2 = model(embeddings_text.to('cpu'), t_q_a_embeddings.to('cpu'), features), torch.stack((normalized_content, normalized_wording), dim=1)\n",
    "\n",
    "\n",
    "for (col1_tensor1, col2_tensor1), (col1_tensor2, col2_tensor2) in zip(tensor1, tensor2):\n",
    "    print(f\"[{col1_tensor1:>6.3f} => {col1_tensor2:>6.3f} = {col1_tensor2-col1_tensor1:>6.3f}] \\t | \\t [{col2_tensor1:>6.3f} => {col2_tensor2:>6.3f} = {col2_tensor2 - col2_tensor1:>6.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30afdbd-630e-4182-97e0-9e0174dbe3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [-0.418 => -0.234 =  0.184] \t | \t [-0.422 => -0.390 =  0.032]\n",
    "# [-0.261 => -0.114 =  0.147] \t | \t [-0.298 => -0.193 =  0.105]\n",
    "# [-0.543 => -0.621 = -0.078] \t | \t [-0.391 => -0.668 = -0.277]\n",
    "# [-0.205 => -0.204 =  0.001] \t | \t [-0.242 => -0.573 = -0.331]\n",
    "# [-0.383 => -0.285 =  0.098] \t | \t [-0.383 => -0.318 =  0.065]\n",
    "# [ 0.008 =>  0.095 =  0.087] \t | \t [-0.194 => -0.167 =  0.027]\n",
    "# [-0.041 =>  0.257 =  0.298] \t | \t [-0.188 => -0.151 =  0.037]\n",
    "# [-0.130 => -0.187 = -0.057] \t | \t [-0.060 =>  0.044 =  0.104]\n",
    "# [-0.370 => -0.464 = -0.094] \t | \t [-0.374 => -0.640 = -0.266]\n",
    "# [-0.434 =>  0.341 =  0.775] \t | \t [-0.396 => -0.182 =  0.214]\n",
    "# [-0.757 => -0.849 = -0.092] \t | \t [-0.694 => -0.820 = -0.126]\n",
    "# [ 0.091 =>  0.250 =  0.159] \t | \t [ 0.057 =>  0.265 =  0.208]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a7665-b682-436b-a62e-a0365521f75a",
   "metadata": {},
   "source": [
    "## Load the Traced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113464e4-6ea3-4ce7-9fd0-2b4074602278",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.jit.load(\"./out/best_model_script.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cdfba-440d-4de0-ac11-22382d876f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (normalized_lexical_density.to(device), \\\n",
    "            normalized_spell_checker.to(device), \\\n",
    "            normalized_tf_idf_question_score.to(device), \\\n",
    "            normalized_avg_word_length.to(device), \\\n",
    "            normalized_smog_index.to(device), \\\n",
    "            normalized_coleman_liau_index.to(device), \\\n",
    "            normalized_flesch_reading_ease.to(device) \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66825f1-b925-463f-bec3-f5a3b20646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model(embeddings_text.to(device), t_q_a_embeddings.to(device), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2099e47-09b4-4fae-8672-c477d5b5bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'a' == 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09318c69-ee9d-4cf4-8dae-14b4a193b034",
   "metadata": {},
   "source": [
    "## export sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc54c-f855-4d24-9337-58e1ad00c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffcf50-3449-4236-aec2-d621488df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e1164-4f09-4d2b-9c1e-b4a496d4778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfffd3d-3a89-49e3-a3fc-71ddc4c2030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317046a-e6ea-414b-9436-8040813ccd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41505048-de86-4300-a07d-1acea9a98eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18a32a-1134-41ff-b4ae-0bda5befa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3d8c6-6c0b-4bb4-b196-20e39d41427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609c6ba-b4f7-4e44-b4e1-4b48c0b6b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b2cc3-98b2-4571-9ac4-8f2b12b5aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(torch.nn.Module):\n",
    "    def __init__(self, repo):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "        self.model = AutoModel.from_pretrained(repo)\n",
    "\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0c373-0bef-421d-8f16-4b6896a26f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95742369-6376-4f15-b761-1c0084706f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_transformer.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffbcd0-68fb-4345-9cbc-2b0783c7f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aeb6dc-c24e-49ac-a4da-0d4b46e458f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
