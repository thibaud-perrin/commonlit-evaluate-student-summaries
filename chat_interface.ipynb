{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d16f40-54f7-4660-9cd6-08b52e3ed64c",
   "metadata": {},
   "source": [
    "# Chat Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77526c-5f53-40b0-999b-06b904c07d64",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52020346-60ed-479e-ab8a-3abc1fb8ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import transformers\n",
    "import torch\n",
    "from torch import bfloat16\n",
    "# from dotenv import load_dotenv  # if you wanted to adapt this for a repo that uses auth\n",
    "from threading import Thread\n",
    "from gradio.themes.utils.colors import Color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49149fe8-a7fa-4e46-a766-182b4641a2da",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d1e65f-3853-400e-bfab-4d40eda462b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 27 13:04:02 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1070      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 34%   45C    P8              11W / 151W |    632MiB /  8192MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3148    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3156    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10972    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11056    C+G   ...ejd91yc\\AdobeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     11104    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12148    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     14000    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15312    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE    N/A      |\n",
      "|    0   N/A  N/A     15780    C+G   ...Brave-Browser\\Application\\brave.exe    N/A      |\n",
      "|    0   N/A  N/A     16468    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     16856    C+G   ...Data\\Local\\Programs\\Blitz\\Blitz.exe    N/A      |\n",
      "|    0   N/A  N/A     17856    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17884    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     20964    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     23612    C+G   ...aam7r\\AcrobatNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     25652    C+G   ...99.0_x64__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c1eedc-4e19-4548-9a8a-2d6915b4092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available()=True\n",
      "torch.cuda.device_count()=1\n",
      "torch.cuda.current_device()=0\n",
      "torch.cuda.device(0)=<torch.cuda.device object at 0x0000022A4A121190>\n",
      "torch.cuda.get_device_name(0)='NVIDIA GeForce GTX 1070'\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "print(f\"{torch.cuda.device_count()=}\")\n",
    "print(f\"{torch.cuda.current_device()=}\")\n",
    "print(f\"{torch.cuda.device(0)=}\")\n",
    "print(f\"{torch.cuda.get_device_name(0)=}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da30a814-a441-4f03-93ae-6e07738d2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Info when using cuda\n",
    "def get_memory_usage():\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6348cf5-3e90-48a7-82a6-4d3e0349d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07592ff1-bebe-4396-bf53-5c3b57c64385",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c9046-fe5b-4bb4-b844-116fec1b712c",
   "metadata": {},
   "source": [
    "### Model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bcb1565-02fb-4e03-b9c4-3ea4b37c38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TheBloke/Llama-2-7b-chat-fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a92ea5-fcff-4672-8c2a-2e2633479aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51312d0-4422-4f1a-b448-6494869db21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f5c459-9196-4cf1-b2de-dda8a33c8198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e3f6682a314ed782736ef72c7b8938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiba\\.virtualenvs\\commonlit-evaluate-student-summaries-bVvbF_GV\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiba\\.virtualenvs\\commonlit-evaluate-student-summaries-bVvbF_GV\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9edf0d-8f22-4414-93f3-67ca9740e2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070\n",
      "Memory Usage:\n",
      "Allocated: 6.0 GB\n",
      "Cached:    6.0 GB\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7227f3d-45bf-4dfe-b6ec-a6c361a2bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_color = \"#FFFFFF\"\n",
    "app_background = \"#0A0A0A\"\n",
    "user_inputs_background = \"#193C4C\"#14303D\"#\"#091820\"\n",
    "widget_bg = \"#000100\"\n",
    "button_bg = \"#141414\"\n",
    "\n",
    "dark = Color(\n",
    "    name=\"dark\",\n",
    "    c50=\"#F4F3EE\",  # not sure\n",
    "    # all text color:\n",
    "    c100=text_color, # Title color, input text color, and all chat text color.\n",
    "    c200=text_color, # Widget name colors (system prompt and \"chatbot\")\n",
    "    c300=\"#F4F3EE\", # not sure\n",
    "    c400=\"#F4F3EE\", # Possibly gradio link color. Maybe other unlicked link colors.\n",
    "    # suggestion text color...\n",
    "    c500=text_color, # text suggestion text. Maybe other stuff.\n",
    "    c600=button_bg,#\"#444444\", # button background color, also outline of user msg.\n",
    "    # user msg/inputs color:\n",
    "    c700=user_inputs_background, # text input background AND user message color. And bot reply outline.\n",
    "    # widget bg.\n",
    "    c800=widget_bg, # widget background (like, block background. Not whole bg), and bot-reply background.\n",
    "    c900=app_background, # app/jpage background. (v light blue)\n",
    "    c950=\"#F4F3EE\", # not sure atm. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3c2a85-e472-4f4f-8d42-2926f17e3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION = \"\"\"\n",
    "# TheBloke/Llama-2-7b-chat-fp16 🗨️\n",
    "This is a streaming Chat Interface implementation of [Llama-2-7B](https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16) \n",
    "for testing inference for a kaggle competition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ef20e8-acfc-4887-995d-8659de0d26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT_EXPLAIN = \"\"\"# System Prompt\n",
    "A system prompt can be used to guide model behavior. See the examples for an idea of this, but feel free to write your own!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5590dea-ffd0-452a-aefe-e12bc1a25ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"You are a helpful AI who only responds by giving two ratings between 0 and 7:\\n\n",
    "      - 'content' to assess whether an answer correctly answers a question.\\n\n",
    "      - \"wording\" to assess the sophistication of the words choose in the answer.\\n\n",
    "      The user will give you the question, the answer and the context used to answer to the question.\\n\n",
    "      Your answer will only be in the following format:\\n\n",
    "      - content: X\\n\n",
    "      - wording: Y\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe1a15d-7c0d-456f-b90c-2a6d817c1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_build(system_prompt, user_inp, hist):\n",
    "    prompt = f\"\"\"### System:\\n{system_prompt}\\n\\n\"\"\"\n",
    "    \n",
    "    for pair in hist:\n",
    "        prompt += f\"\"\"### User:\\n{pair[0]}\\n\\n### Assistant:\\n{pair[1]}\\n\\n\"\"\"\n",
    "\n",
    "    prompt += f\"\"\"### User:\\n{user_inp}\\n\\n### Assistant:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f7d719-2728-4b4c-a33f-aae1e207018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_build(system_prompt, user_inp, hist):\n",
    "    prompt = f\"\"\"### System:\\n{system_prompt}\\n\\n\"\"\"\n",
    "    \n",
    "    for pair in hist:\n",
    "        prompt += f\"\"\"### User:\\n{pair[0]}\\n\\n### Assistant:\\n{pair[1]}\\n\\n\"\"\"\n",
    "\n",
    "    prompt += f\"\"\"### User:\\n{user_inp}\\n\\n### Assistant:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def chat(user_input, history, system_prompt):\n",
    "\n",
    "    prompt = prompt_build(system_prompt, user_input, history)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=120., skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        #max_new_tokens=512, # will override \"max_len\" if set.\n",
    "        max_length=2048,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        top_k=50\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    model_output = \"\"\n",
    "    for new_text in streamer:\n",
    "        model_output += new_text\n",
    "        yield model_output\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6e9a7f-2721-4c7a-999c-23e9aac34738",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Monochrome(\n",
    "               font=[gr.themes.GoogleFont(\"Montserrat\"), \"Arial\", \"sans-serif\"],\n",
    "               primary_hue=\"sky\",  # when loading\n",
    "               secondary_hue=\"sky\", # something with links\n",
    "               neutral_hue=\"dark\"),) as demo:  #main.\n",
    "\n",
    "    gr.Markdown(DESCRIPTION)\n",
    "    gr.Markdown(SYS_PROMPT_EXPLAIN)\n",
    "    dropdown = gr.Dropdown(choices=prompts, label=\"Type your own or select a system prompt\", value=\"You are a helpful AI.\", allow_custom_value=True)\n",
    "    chatbot = gr.ChatInterface(fn=chat, additional_inputs=[dropdown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405cb66-4f89-4e0d-869c-22d0f6a47ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo.queue(api_open=False).launch(show_api=False,share=False, debug=True, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b9ff8-b779-43d2-80ed-180fa51d5ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e2eff-160c-4c22-aa68-6f80b29b40cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784c506-a157-41ac-88ae-b51d5e590212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e08976-90e0-4237-a37f-98abee1355fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question:\n",
    "- Summarize at least 3 elements of an ideal tragedy, as described by Aristotle.\n",
    "\n",
    "Answer:\n",
    "- 1 element of an ideal tragedy is that it should be arranged on a complex plan. Another element of an ideal tragedy is that it should only have one main issue. The last element of an ideal tragedy is that it should have a double thread plot and an opposite catastrophe for both good and bad.\n",
    "\n",
    "Context:\n",
    "- Chapter 13 \\r\\nAs the sequel to what has already been said, we must proceed to consider what the poet should aim at, and what he should avoid, in constructing his plots; and by what means the specific effect of Tragedy will be produced. \\r\\nA perfect tragedy should, as we have seen, be arranged not on the simple but on the complex plan. It should, moreover, imitate actions which excite pity and fear, this being the distinctive mark of tragic imitation. It follows plainly, in the first place, that the change of fortune presented must not be the spectacle of a virtuous man brought from prosperity to adversity: for this moves neither pity nor fear; it merely shocks us. Nor, again, that of a bad man passing from adversity to prosperity: for nothing can be more alien to the spirit of Tragedy; it possesses no single tragic quality; it neither satisfies the moral sense nor calls forth pity or fear. Nor, again, should the downfall of the utter villain be exhibited. A plot of this kind would, doubtless, satisfy the moral sense, but it would inspire neither pity nor fear; for pity is aroused by unmerited misfortune, fear by the misfortune of a man like ourselves. Such an event, therefore, will be neither pitiful nor terrible. There remains, then, the character between these two extremes — that of a man who is not eminently good and just, yet whose misfortune is brought about not by vice or depravity, but by some error of judgement or frailty. He must be one who is highly renowned and prosperous — a personage like Oedipus, Thyestes, or other illustrious men of such families. \\r\\nA well-constructed plot should, therefore, be single in its issue, rather than double as some maintain. The change of fortune should be not from bad to good, but, reversely, from good to bad. It should come about as the result not of vice, but of some great error or frailty, in a character either such as we have described, or better rather than worse. The practice of the stage bears out our view. At first the poets recounted any legend that came in their way. Now, the best tragedies are founded on the story of a few houses — on the fortunes of Alcmaeon, Oedipus, Orestes, Meleager, Thyestes, Telephus, and those others who have done or suffered something terrible. A tragedy, then, to be perfect according to the rules of art, should be of this construction. Hence they are in error who censure Euripides just because he follows this principle in his plays, many of which end unhappily. It is, as we have said, the right ending. The best proof is that on the stage and in dramatic competition, such plays, if well worked out, are the most tragic in effect; and Euripides, faulty though he may be in the general management of his subject, yet is felt to be the most tragic of the poets. \\r\\nIn the second rank comes the kind of tragedy which some place first. Like the Odyssey, it has a double thread of plot, and also an opposite catastrophe for the good and for the bad. It is accounted the best because of the weakness of the spectators; for the poet is guided in what he writes by the wishes of his audience. The pleasure, however, thence derived is not the true tragic pleasure. It is proper rather to Comedy, where those who, in the piece, are the deadliest enemies — like Orestes and Aegisthus — quit the stage as friends at the close, and no one slays or is slain.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
